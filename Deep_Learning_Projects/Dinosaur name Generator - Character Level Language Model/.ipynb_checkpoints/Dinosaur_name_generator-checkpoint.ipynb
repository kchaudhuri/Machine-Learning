{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'A list of dinosaurs.', 'dinosaurs': ['Kangnasaurus', 'Lophostropheus', 'Spinophorosaurus', 'Epachthosaurus', 'Coelurosauria', 'Lycorhinus', 'Adasaurus', 'Draconyx', 'Ceratops', 'Lagerpeton', 'Qiaowanlong', 'Rhynchosaur', 'Ningyuansaurus', 'Palaeolimnornis', 'Anabisetia', 'Talarurus', 'Sphenodontia', 'Tianyulong', 'Aepisaurus', 'Neuquenraptor', 'Galesaurus', 'Pachysuchus', 'Auroraceratops', 'Hecatasaurus', 'Barapasaurus', 'Asiatosaurus', 'Daanosaurus', 'Luoyanggia', 'Eobrontosaurus', 'Wellnhoferia', 'Zuolong', 'Tenchisaurus', 'Centrosaurus', 'Falcarius', 'Ojoraptorsaurus', 'Lufengocephalus', 'Vulcanodon', 'Mollusc', 'Shunosaurus', 'Empaterias', 'Issasaurus', 'Jiangxisaurus', 'Laplatasaurus', 'Aublysodon', 'Embasaurus', 'Blikanasaurus', 'Bonapartenykus', 'Asiamericana', 'Arizonasaurus', 'Cryptoraptor', 'Morosaurus', 'Mapusaurus', 'Buitreraptor', 'Gravitholus', 'Vitakrisaurus', 'Nurosaurus', 'Cetiosauriscus', 'Kukufeldia', 'Propanoplosaurus', 'Suchoprion', 'Umarsaurus', 'Likhoelesaurus', 'Dasygnathoides', 'Dubreuillosaurus', 'Shenzhouraptor', 'Atrociraptor', 'Amurosaurus', 'Latirhinus', 'Albisaurus', 'Zupaysaurus', 'Venenosaurus', 'Syrmosaurus', 'Arkharavia', 'Yueosaurus', 'Tianchungosaurus', 'Dyslocosaurus', 'Sinraptor', 'Owenodon', 'Pelycosaur', 'Duriavenator', 'Haplocheirus', 'Epidendrosaurus', 'Nyasasaurus', 'Tawasaurus', 'Lisboasaurus', 'Ahshislepelta', 'Magnirostris', 'Psittacosaurus', 'Orodromeus', 'Ostafrikasaurus', 'Nemegtomaia', 'Dracorex', 'Ovoraptor', 'Amazonsaurus', 'Leyesaurus', 'Dollodon', 'Cryptosaurus', 'Coahuilaceratops', 'Variraptor', 'Panamericansaurus', 'Nodocephalosaurus', 'Nasutoceratops', 'Prosaurolophus', 'Jingshanosaurus', 'Teleocrater', 'Tribelesodon', 'Mochlodon', 'Eohadrosaurus', 'Comanchesaurus', 'Limnornis', 'Gigantspinosaurus', 'Oxalaia', 'Wuerhosaurus', 'Mtapaiasaurus', 'Longisquama', 'Velocisaurus', 'Aorun', 'Sinopliosaurus', 'Gigantosaurus', 'Sphenospondylus', 'Dinotyrannus', 'Rhabdodon', 'Kritosaurus', 'Lamplughsaura', 'Notohypsilophodon', 'Tsagantegia', 'Brontoraptor', 'Argyrosaurus', 'Lambeosaurus', 'Heterosaurus', 'Tazoudasaurus', 'Valdoraptor', 'Microhadrosaurus', 'Pycnonemosaurus', 'Manidens', 'Coelophysis', 'Hulsanpes', 'Losillasaurus', 'Polacanthoides', 'Lanzhousaurus', 'Walgettosuchus', 'Sauroniops', 'Magulodon', 'Pneumatoraptor', 'Altispinax', 'Alnashetri', 'Hoplitosaurus', 'Rahiolisaurus', 'Luanpingosaurus', 'Abrosaurus', 'Palaeosaurus', 'Iguanoides', 'Abydosaurus', 'Riodevasaurus', 'Stormbergia', 'Bihariosaurus', 'Yuanmousaurus', 'Sphenosuchus', 'Dashanpusaurus', 'Crocodilia', 'Enigmosaurus', 'Montanoceratops', 'Frenguellisaurus', 'Segnosaurus', 'Kryptops', 'Labocania', 'Sinocalliopteryx', 'Dromiceiomimus', 'Isisaurus', 'Archaeornithoides', 'Deinonychus', 'Allosaurus', 'Stephanosaurus', 'Sinornithoides', 'Incisivosaurus', 'Ornitholestes', 'Ankylosaurus', 'Plateosaurus', 'Shidaisaurus', 'Platyceratops', 'Agnosphitys', 'Changdusaurus', 'Nothosaur', 'Orthogoniosaurus', 'Titanoceratops', 'Dysganus', 'Lamaceratops', 'Helioceratops', 'Nanyangosaurus', 'Khaan', 'Cryptodraco', 'Chasmosaurus', 'Rileyasuchus', 'Aeolosaurus', 'Yizhousaurus', 'Echinodon', 'Omnivoropteryx', 'Lengosaurus', 'Megadactylus', 'Mamenchisaurus', 'Notoceratops', 'Pachysaurus', 'Tianzhenosaurus', 'Zhuchengceratops', 'Tyreophorus', 'Nedcolbertia', 'Shixinggia', 'Jeholosaurus', 'Ornithosuchus', 'Veterupristisaurus', 'Rayososaurus', 'Velafrons', 'Lapparentosaurus', 'Seismosaurus', 'Tsuchikurasaurus', 'Styracosaurus', 'Dilophosaurus', 'Gasparinisaura', 'Xiaotingia', 'Dromaeosaurus', 'Scansoriopteryx', 'Eurolimnornis', 'Proterosuchid', 'Nipponosaurus', 'Brachiosaurus', 'Vitaridrinda', 'Mantellodon', 'Sphaerotholus', 'Shuosaurus', 'Koutalisaurus', 'Cardiodon', 'Yixianosaurus', 'Jurassosaurus', 'Jiutaisaurus', 'Gryphognathus', 'Archaeornithomimus', 'Griphornis', 'Szechuanosaurus', 'Pneumatoarthrus', 'Basutodon', 'Aletopelta', 'Tendaguria', 'Riojasaurus', 'Creosaurus', 'Harpymimus', 'Huaxiagnathus', 'Carnotaurus', 'Beipiaosaurus', 'Teratophoneus', 'Cedarosaurus', 'Omosaurus', 'Wyleyia', 'Aachenosaurus', 'Atlantosaurus', 'Bilbeyhallorum', 'Balochisaurus', 'Ouranosaurus', 'Fukuititan', 'Sarcolestes', 'Alocodon', 'Amphisaurus', 'Lametasaurus', 'Raptorex', 'Sinosaurus', 'Doratodon', 'Ankistrodon', 'Chuanjiesaurus', 'Parasaurolophus', 'Albertonykus', 'Efraasia', 'Alaskacephale', 'Torilion', 'Rapator', 'Sauroplites', 'Thecocoelurus', 'Lirainosaurus', 'Xixiasaurus', 'Conchoraptor', 'Paralititan', 'Arcusaurus', 'Becklespinax', 'Thecodontosaurus', 'Taveirosaurus', 'Daemonosaurus', 'Suuwassea', 'Albertosaurus', 'Nouerosaurus', 'Marshosaurus', 'Xuanhuasaurus', 'Algoasaurus', 'Capitalsaurus', 'Xenoposeidon', 'Cryolophosaurus', 'Gobipteryx', 'Stereosaurus', 'Nemegtia', 'Proyandusaurus', 'Thyreophora', 'Xiaosaurus', 'Tatankaceratops', 'Hanwulosaurus', 'Gryphoceratops', 'Linheraptor', 'Ornithomimoides', 'Edmontonia', 'Aerosteon', 'Ninghsiasaurus', 'Prenoceratops', 'Vagaceratops', 'Sinucerasaurus', 'Hongshanosaurus', 'Hexinlusaurus', 'Indosuchus', 'Moshisaurus', 'Alashansaurus', 'Berberosaurus', 'Elaltitan', 'Marisaurus', 'Rhoetosaurus', 'Tatankacephalus', 'Bienosaurus', 'Dracopelta', 'Chiayusaurus', 'Pseudosuchia', 'Sarcosaurus', 'Shuvuuia', 'Unaysaurus', 'Amtosaurus', 'Nqwebasaurus', 'Claorhynchus', 'Prolacertiform', 'Honghesaurus', 'Ugrosaurus', 'Aegyptosaurus', 'Gallimimus', 'Clasmodosaurus', 'Hypacrosaurus', 'Caenagnathus', 'Velocipes', 'Lessemsaurus', 'Agrosaurus', 'Paronychodon', 'Maleevosaurus', 'Leipsanosaurus', 'Clevelanotyrannus', 'Dynamosaurus', 'Megacervixosaurus', 'Protohadros', 'Polyonax', 'Daxiatitan', 'Spondylosoma', 'Ichthyovenator', 'Demandasaurus', 'Dimodosaurus', 'Torvosaurus', 'Gwyneddosaurus', 'Cystosaurus', 'Irritator', 'Zanclodon', 'Rugops', 'Ignavusaurus', 'Chinshakiangosaurus', 'Zhejiangosaurus', 'Pachyrhinosaurus', 'Stenotholus', 'Iuticosaurus', 'Tyrannotitan', 'Xixianykus', 'Palaeopteryx', 'Vitakridrinda', 'Planicoxa', 'Jianchangosaurus', 'Sinovenator', 'Ohmdenosaurus', 'Protecovasaurus', 'Eoceratops', 'Laevisuchus', 'Cumnoria', 'Ratchasimasaurus', 'Elaphrosaurus', 'Dracovenator', 'Abelisaurus', 'Sangonghesaurus', 'Austrocheirus', 'Calamosaurus', 'Vectensia', 'Elosaurus', 'Termatosaurus', 'Pleuropeltus', 'Chubutisaurus', 'Macrophalangia', 'Futalongkosaurus', 'Acristavus', 'Wintonotitan', 'Diclonius', 'Nanosaurus', 'Tonganosaurus', 'Tarascosaurus', 'Amphicoelicaudia', 'Achillesaurus', 'Delapparentia', 'Argentinosaurus', 'Sulaimansaurus', 'Koparion', 'Brachytrachelopan', 'Bakesaurus', 'Rahona', 'Oryctodromeus', 'Campylodon', 'Stygivenator', 'Wangonisaurus', 'Genyodectes', 'Acrocanthosaurus', 'Danubiosaurus', 'Deltadromeus', 'Rileya', 'Borealosaurus', 'Rioarribasaurus', 'Gondwanatitan', 'Lophorhothon', 'Talenkauen', 'Diracodon', 'Nanshiungosaurus', 'Bradycneme', 'Ferganocephale', 'Cheneosaurus', 'Wulagasaurus', 'Tanystrosuchus', 'Comahuesaurus', 'Actiosaurus', 'Jiangjunmiaosaurus', 'Xinjiangovenator', 'Gadolosaurus', 'Clarencea', 'Avemetatarsalia', 'Dakotadon', 'Diapsid', 'Albinykus', 'Pectinodon', 'Protorosaurus', 'Ginnareemimus', 'Doryphorosaurus', 'Dalianraptor', 'Megapnosaurus', 'Scelidosaurus', 'Metriorhynchid', 'Piatnitzkysaurus', 'Haplocanthosaurus', 'Phytosaur', 'Manospondylus', 'Gansutitan', 'Neovenator', 'Brasileosaurus', 'Judiceratops', 'Khetranisaurus', 'Fish', 'Proceratosaurus', 'Zatomus', 'Ceratosaurus', 'Unescoceratops', 'Telmatosaurus', 'Segisaurus', 'Pachyspondylus', 'Caseosaurus', 'Ultrasauros', 'Dongbeititan', 'Galvesaurus', 'Crocodylomorph', 'Ephoenosaurus', 'Fossil', 'Crosbysaurus', 'Coelosaurus', 'Unenlagia', 'Strenusaurus', 'Concavenator', 'Fukuiraptor', 'Camarasaurus', 'Iliosuchus', 'Huayangosaurus', 'Kileskus', 'Clepsysaurus', 'Richardoestesia', 'Sphenosaurus', 'Scutellosaurus', 'Garudimimus', 'Hexing', 'Nanningosaurus', 'Sonorasaurus', 'Pradhania', 'Orosaurus', 'Andesaurus', 'Genusaurus', 'Huxleysaurus', 'Elopteryx', 'Alectrosaurus', 'Tecovasaurus', 'Parksosaurus', 'Paranthodon', 'Airakoraptor', 'Jobaria', 'Ichabodcraniosaurus', 'Muyelensaurus', 'Sacisaurus', 'Deinodon', 'Patricosaurus', 'Maleevus', 'Tylocephale', 'Sugiyamasaurus', 'Nodosaurus', 'Aliwalia', 'Kerberosaurus', 'Kazaklambia', 'Eolambia', 'Dongyangosaurus', 'Citipati', 'Euskelosaurus', 'Trigonosaurus', 'Epidexipteryx', 'Dolichosuchus', 'Walkeria', 'Labrosaurus', 'Condorraptor', 'Tichosteus', 'Uberabatitan', 'Magnosaurus', 'Janenschia', 'Anasazisaurus', 'Macrogryphosaurus', 'Erliansaurus', 'Ornithotarsus', 'Bayosaurus', 'Santanaraptor', 'Zhuchengtyrannus', 'Lukousaurus', 'Sauroposeidon', 'Ampelosaurus', 'Pampadromaeus', 'Erectopus', 'Glyptodontopelta', 'Drinker', 'Leaellynasaura', 'Magyarosaurus', 'Postosuchus', 'Szechuanoraptor', 'Yubasaurus', 'Brachyrophus', 'Cionodon', 'Sellacoxa', 'Elachistosuchus', 'Shuvosaurus', 'Sauraechmodon', 'Microdontosaurus', 'Carcharodontosaurus', 'Brachylophosaurus', 'Theropoda', 'Tapinocephalus', 'Changchunsaurus', 'Cladeiodon', 'Pareiasaurus', 'Heishansaurus', 'Aristosuchus', 'Protiguanodon', 'Brohisaurus', 'Eupodosaurus', 'Datousaurus', 'Giraffatitan', 'Jaklapallisaurus', 'Tugulusaurus', 'Compsognathus', 'Ilokelesia', 'Revueltoraptor', 'Tuojiangosaurus', 'Huaxiasaurus', 'Palaeocursornis', 'Onychosaurus', 'Ceratonykus', 'Amargatitanis', 'Albalophosaurus', 'Byronosaurus', 'Cryptovolans', 'Shenzhousaurus', 'Rapetosaurus', 'Altirhinus', 'Sanjuansaurus', 'Dysalotosaurus', 'Archaeopteryx', 'Liliensternus', 'Beelemodon', 'Xuanhuaceratops', 'Protrachodon', 'Caenagnathasia', 'Willinakaqe', 'Atacamatitan', 'Lourinhanosaurus', 'Yaverlandia', 'Ligomasaurus', 'Suchomimus', 'Brasilotitan', 'Jenghizkhan', 'Aggiosaurus', 'Elrhazosaurus', 'Yingshanosaurus', 'Australovenator', 'Ichthyornis', 'Valdosaurus', 'Yuanmouraptor', 'Prolacertiformes', 'Tarchia', 'Hesperosaurus', 'Azendohsaurus', 'Eucentrosaurus', 'Scipionyx', 'Petrobrasaurus', 'Hudiesaurus', 'Sinornithosaurus', 'Shuangmiaosaurus', 'Tianchisaurus', 'Araucanoraptor', 'Poposaurus', 'Pararhabdodon', 'Osmakasaurus', 'Siamotyrannus', 'Galveosaurus', 'Yangchuanosaurus', 'Microcephale', 'Mirischia', 'Probactrosaurus', 'Yunxiansaurus', 'Enantiornithine', 'Cedrorestes', 'Chaoyangsaurus', 'Loricatosaurus', 'Stygimoloch', 'Venaticosuchus', 'Gorgosaurus', 'Anchiornis', 'Ischisaurus', 'Bactrosaurus', 'Quilmesaurus', 'Fukuisaurus', 'Stegosaurus', 'Griphosaurus', 'Graciliceratops', 'Oligosaurus', 'Baotianmansaurus', 'Eocursor', 'Turtle', 'Macelognathus', 'Arctosaurus', 'Streptospondylus', 'Texasetes', 'Dianchungosaurus', 'Birds', 'Nomingia', 'Sinornithomimus', 'Hierosaurus', 'Abdallahsaurus', 'Jaxartosaurus', 'Sanchusaurus', 'Tarbosaurus', 'Sinocoelurus', 'Timimus', 'Herrerasaurus', 'Giganotosaurus', 'Tsaagan', 'Anthracothere', 'Yezosaurus', 'Chihuahuasaurus', 'Cathartesaura', 'Domeykosaurus', 'Loncosaurus', 'Archaeoceratops', 'Udanoceratops', 'Gojirasaurus', 'Dongyangopelta', 'Duriatitan', 'Chondrosteus', 'Arkanosaurus', 'Lusotitan', 'Diceratus', 'Nuoersaurus', 'Wadhurstia', 'Sauropelta', 'Ultrasaurus', 'Indosaurus', 'Kotasaurus', 'Urbacodon', 'Zhongyuansaurus', 'Tanystropheus', 'Mifunesaurus', 'Pegomastax', 'Gobititan', 'Lusitanosaurus', 'Caulodon', 'Baryonyx', 'Tataouinea', 'Longosaurus', 'Pareiasaur', 'Leshansaurus', 'Machairasaurus', 'Colossosaurus', 'Arstanosaurus', 'Wakinosaurus', 'Lanasaurus', 'Dryosaurus', 'Sterrholophus', 'Dachungosaurus', 'Chialingosaurus', 'Jiangjunosaurus', 'Klamelisaurus', 'Rauisuchia', 'Caudipteryx', 'Jinfengopteryx', 'Leptoceratops', 'Sinotyrannus', 'Poekilopleuron', 'Gobisaurus', 'Angaturama', 'Pterosaur', 'Aetonyx', 'Angolatitan', 'Tanius', 'Therizinosaurus', 'Tastavinsaurus', 'Lancangosaurus', 'Dinheirosaurus', 'Brachypodosaurus', 'Gobiceratops', 'Fruitadens', 'Monkonosaurus', 'Spinostropheus', 'Cristatusaurus', 'Bolong', 'Rebbachisaurus', 'Monolophosaurus', 'Alioramus', 'Hypsirophus', 'Olorotitan', 'Euoplocephalus', 'Juravenator', 'Euhelopus', 'Epanterias', 'Lancanjiangosaurus', 'Xianshanosaurus', 'Ekrixinatosaurus', 'Achillobator', 'Hesperonychus', 'Pachysauriscus', 'Ruyangosaurus', 'Gongbusaurus', 'Teratosaurus', 'Ruehleia', 'Yaleosaurus', 'Canardia', 'Brachyceratops', 'Pantydraco', 'Afrovenator', 'Mendozasaurus', 'Pedopenna', 'Blasisaurus', 'Astrodon', 'Mandschurosaurus', 'Drusilasaura', 'Libycosaurus', 'Eucamerotus', 'Tonouchisaurus', 'Didanodon', 'Proplanicoxa', 'Kentrurosaurus', 'Trinisaura', 'Fusuisaurus', 'Kentrosaurus', 'Eocarcharia', 'Albertaceratops', 'Rutellum', 'Saltasaurus', 'Majungatholus', 'Mussaurus', 'Zigongosaurus', 'Euacanthus', 'Craspedodon', 'Ingenia', 'Priodontognathus', 'Rubeosaurus', 'Gyposaurus', 'Utahraptor', 'Pukyongosaurus', 'Coelurosaur', 'Silvisaurus', 'Troodon', 'Jixiangornis', 'Pawpawsaurus', 'Oohkotokia', 'Hadrosauravus', 'Shaochilong', 'Ponerosteus', 'Ischyrosaurus', 'Hadrosaurus', 'Gryposaurus', 'Spinops', 'Peloroplites', 'Daspletosaurus', 'Dravidosaurus', 'Hagryphus', 'Sphenosuchia', 'Ligabueino', 'Mymoorapelta', 'Tatisaurus', 'Trimucrodon', 'Cathetosaurus', 'Teinurosaurus', 'Antetonitrus', 'Rajasaurus', 'Fabrosaurus', 'Angloposeidon', 'Levnesovia', 'Mongolosaurus', 'Asiaceratops', 'Avipes', 'Turiasaurus', 'Eucnemesaurus', 'Otogosaurus', 'Martharaptor', 'Tsintaosaurus', 'Hypsilophodon', 'Gigantoscelus', 'Palaeosauriscus', 'Hironosaurus', 'Paludititan', 'Anatosaurus', 'Kaatedocus', 'Linhevenator', 'Pellegrinisaurus', 'Sanpasaurus', 'Lapampasaurus', 'Inosaurus', 'Eomamenchisaurus', 'Liassaurus', 'Jinzhousaurus', 'Equijubus', 'Dryptosaurus', 'Nopcsaspondylus', 'Changtusaurus', 'Tapuiasaurus', 'Diabloceratops', 'Dakosaurus', 'Chassternbergia', 'Limaysaurus', 'Huaxiaosaurus', 'Minotaurasaurus', 'Uteodon', 'Micropachycephalosaurus', 'Avisaurus', 'Siamodon', 'Ornithomerus', 'Eodromaeus', 'Turanoceratops', 'Nambalia', 'Cruxicheiros', 'Riojasuchus', 'Stokesosaurus', 'Amygdalodon', 'Linhenykus', 'Heterodontosaurus', 'Dromicosaurus', 'Bahariasaurus', 'Xiongguanlong', 'Jeyawati', 'Polyodontosaurus', 'Morinosaurus', 'Campylodoniscus', 'Aralosaurus', 'Pentaceratops', 'Squalodon', 'Saurornitholestes', 'Yandusaurus', 'Bruhathkayosaurus', 'Kinnareemimus', 'Tornieria', 'Scaphonyx', 'Barosaurus', 'Titanosaurus', 'Volkheimeria', 'Brontomerus', 'Megalosaurus', 'Rhopalodon', 'Huanghetitan', 'Ngexisaurus', 'Jiangshanosaurus', 'Archosaur', 'Sciurumimus', 'Histriasaurus', 'Spinosaurus', 'Eoraptor', 'Phaedrolosaurus', 'Betasuchus', 'Belodon', 'Bagaraatan', 'Protognathus', 'Marmarospondylus', 'Dinosaur', 'Darwinsaurus', 'Apatodon', 'Eotyrannus', 'Struthiomimus', 'Pelecanimimus', 'Gideonmantellia', 'Appalachiosaurus', 'Edmontosaurus', 'Acracanthus', 'Machimosaurus', 'Razanandrongobe', 'Uintasaurus', 'Magnapaulia', 'Moabosaurus', 'Ojoceratops', 'Yaxartosaurus', 'Macroscelosaurus', 'Aniksosaurus', 'Guanlong', 'Futabasaurus', 'Proceratops', 'Xuwulong', 'Ricardoestesia', 'Symphyrophus', 'Pleurocoelus', 'Damalasaurus', 'Naashoibitosaurus', 'Panphagia', 'Graciliraptor', 'Ornithoides', 'Futalognkosaurus', 'Pachycephalosaurus', 'Plesiosaur', 'Astrophocaudia', 'Gilmoreosaurus', 'Diamantinasaurus', 'Eucercosaurus', 'Gavinosaurus', 'Suchosaurus', 'Eugongbusaurus', 'Camelotia', 'Shanxia', 'Antarctopelta', 'Traukutitan', 'Saurornithoides', 'Hippodraco', 'Nteregosaurus', 'Sauropodus', 'Coelurosauravus', 'Stereocephalus', 'Adeopapposaurus', 'Ctenosauriscid', 'Paraiguanodon', 'Eosinopteryx', 'Archaeornis', 'Bambiraptor', 'Isanosaurus', 'Stegosaurides', 'Avaceratops', 'Gannansaurus', 'Triceratops', 'Paluxysaurus', 'Denversaurus', 'Bathygnathus', 'Katsuyamasaurus', 'Barsboldia', 'Mojoceratops', 'Eucoelophysis', 'Microraptor', 'Calamospondylus', 'Fusinasus', 'Agathaumas', 'Protoceratops', 'Eustreptospondylus', 'Macrurosaurus', 'Atlascopcosaurus', 'Claosaurus', 'Avicephala', 'Tochisaurus', 'Protognathosaurus', 'Kakuru', 'Neuquensaurus', 'Chilantaisaurus', 'Chindesaurus', 'Parvicursor', 'Phuwiangosaurus', 'Saltriosaurus', 'Edmarka', 'Chingkankousaurus', 'Bicentenaria', 'Kemkemia', 'Microceratus', 'Chaoyangosaurus', 'Atlasaurus', 'Eotriceratops', 'Neimongosaurus', 'Unicerosaurus', 'Qinlingosaurus', 'Laosaurus', 'Craterosaurus', 'Dacentrurus', 'Geminiraptor', 'Erlikosaurus', 'Bonitasaura', 'Arrhinoceratops', 'Triassolestes', 'Hisanohamasaurus', 'Oplosaurus', 'Supersaurus', 'Achelousaurus', 'Rinconsaurus', 'Serendipaceratops', 'Eoabelisaurus', 'Dystrophaeus', 'Thotobolosaurus', 'Blancocerosaurus', 'Cetiosaurus', 'Roccosaurus', 'Chondrosteosaurus', 'Fulengia', 'Mohammadisaurus', 'Halticosaurus', 'Lancangjiangosaurus', 'Teyuwasu', 'Breviceratops', 'Rhodanosaurus', 'Metriacanthosaurus', 'Overosaurus', 'Newtonsaurus', 'Gracilisuchus', 'Pseudolagosuchus', 'Sellosaurus', 'Ajkaceratops', 'Lufengosaurus', 'Yamaceratops', 'Bagaceratops', 'Apatosaurus', 'Khateranisaurus', 'Batyrosaurus', 'Cinizasaurus', 'Tanycolagreus', 'Bainoceratops', 'Silesaurid', 'Byranjaffia', 'Shamosaurus', 'Austrosaurus', 'Amtocephale', 'Cylindricodon', 'Huabeisaurus', 'Regnosaurus', 'Callovosaurus', 'Heptasteornis', 'Gryponyx', 'Tylosteus', 'Gargoyleosaurus', 'Kundurosaurus', 'Mononykus', 'Secernosaurus', 'Puertasaurus', 'Bellusaurus', 'Archosauriform', 'Qiupalong', 'Alamosaurus', 'Dystylosaurus', 'Tethyshadros', 'Selimanosaurus', 'Albertadromeus', 'Karongasaurus', 'Kaijiangosaurus', 'Microvenator', 'Amphicoelias', 'Eshanosaurus', 'Salimosaurus', 'Avimimus', 'Elvisaurus', 'Bonatitan', 'Procheneosaurus', 'Tetragonosaurus', 'Mammal', 'Oshanosaurus', 'Jurapteryx', 'Paleosaurus', 'Dinosauromorph', 'Plateosauravus', 'Hungarosaurus', 'Chromogisaurus', 'Anoplosaurus', 'Amargasaurus', 'Merosaurus', 'Alwalkeria', 'Macrodontophion', 'Shanshanosaurus', 'Gongxianosaurus', 'Vectisaurus', 'Kitadanisaurus', 'Agustinia', 'Sulaimanisaurus', 'Thescelosaurus', 'Zephyrosaurus', 'Arenysaurus', 'Patagosaurus', 'Laornis', 'Protoavis', 'Tenantosaurus', 'Yinlong', 'Bravoceratops', 'Saurophaganax', 'Coeluroides', 'Seitaad', 'Trialestes', 'Glishades', 'Cedarpelta', 'Rinchenia', 'Lesothosaurus', 'Dromaeosaur', 'Nuoerosaurus', 'Hallopus', 'Yanornis', 'Tomodon', 'Juratyrant', 'Shanyangosaurus', 'Deuterosaurus', 'Coelurus', 'Zizhongosaurus', 'Hypselospinus', 'Austroraptor', 'Brachytaenius', 'Orcomimus', 'Acrotholus', 'Chirostenotes', 'Emausaurus', 'Nigersaurus', 'Animantarx', 'Luanchuanraptor', 'Zuniceratops', 'Wannanosaurus', 'Abrictosaurus', 'Heyuannia', 'Synapsid', 'Struthiosaurus', 'Yunnanosaurus', 'Noasaurus', 'Tyrannosaurid', 'Yibinosaurus', 'Thespesius', 'Priconodon', 'Marasuchus', 'Anchisaurus', 'Iguanacolossus', 'Patagonykus', 'Haplocanthus', 'Corythosaurus', 'Limusaurus', 'Coloradisaurus', 'Similicaudipteryx', 'Leptospondylus', 'Aviatyrannis', 'Hortalotarsus', 'Tienshanosaurus', 'Pamparaptor', 'Peishansaurus', 'Succinodon', 'Dinosaurus', 'Rachitrema', 'Dachongosaurus', 'Therapsida', 'Saurolophus', 'Pekinosaurus', 'Skorpiovenator', 'Microsaurops', 'Ammosaurus', 'Ichthyosaur', 'Antarctosaurus', 'Liaoningosaurus', 'Nothronychus', 'Spinosuchus', 'Iguanosaurus', 'Anchiceratops', 'Orkoraptor', 'Microcoelus', 'Xuanhanosaurus', 'Geranosaurus', 'Goyocephale', 'Ankylosauria', 'Dahalokely', 'Daptosaurus', 'Loricosaurus', 'Australodocus', 'Pterospondylus', 'Gigantoraptor', 'Hylaeosaurus', 'Caudocoelus', 'Opisthocoelicaudia', 'Rhadinosaurus', 'Ligabuesaurus', 'Aragosaurus', 'Rocasaurus', 'Parrosaurus', 'Sahaliyania', 'Agilisaurus', 'Orthomerus', 'Penelopognathus', 'Liaoceratops', 'Dyoplosaurus', 'Stenonychosaurus', 'Walkersaurus', 'Narambuenatitan', 'Pitekunsaurus', 'Camarillasaurus', 'Procolophonid', 'Epichirostenotes', 'Beishanlong', 'Gasosaurus', 'Coronosaurus', 'Zapalasaurus', 'Sauraechinodon', 'Glacialisaurus', 'Therapsid', 'Gravisaurus', 'Lewisuchus', 'Jainosaurus', 'Centemodon', 'Arkansaurus', 'Einiosaurus', 'Palaeoscincus', 'Cerasinops', 'Microceratops', 'Pteropelyx', 'Sigilmassasaurus', 'Staurikosaurus', 'Pyroraptor', 'Ornithodesmus', 'Asylosaurus', 'Anodontosaurus', 'Siluosaurus', 'Carnosauria', 'Qantassaurus', 'Ornithomimus', 'Dinodocus', 'Chuxiongosaurus', 'Anserimimus', 'Camptonotus', 'Lagosuchus', 'Anatotitan', 'Dryptosauroides', 'Silesaurus', 'Pachysaurops', 'Saichania', 'Procerosaurus', 'Lucianosaurus', 'Koreasaurus', 'Lurdusaurus', 'Piveteausaurus', 'Jeholornis', 'Angulomastacator', 'Hoplosaurus', 'Saltopus', 'Shanag', 'Homalocephale', 'Torosaurus', 'Fenestrosaurus', 'Jubbulpuria', 'Mtotosaurus', 'Nemegtosaurus', 'Maiasaura', 'Yutyrannus', 'Ozraptor', 'Lariosaurus', 'Bugenasaura', 'Ornatotholus', 'Aardonyx', 'Herbstosaurus', 'Tianyuraptor', 'Diplodocus', 'Kosmoceratops', 'Muttaburrasaurus', 'Kagasaurus', 'Atsinganosaurus', 'Massospondylus', 'Utahceratops', 'Nanotyrannus', 'Qingxiusaurus', 'Avalonianus', 'Erketu', 'Fulgurotherium', 'Kayentavenator', 'Protarchaeopteryx', 'Chungkingosaurus', 'Krzyzanowskisaurus', 'Yimenosaurus', 'Aurornis', 'Ganzhousaurus', 'Pinacosaurus', 'Archosauromorph', 'Niobrarasaurus', 'Melanorosaurus', 'Baurutitan', 'Elmisaurus', 'Euronychodon', 'Tehuelchesaurus', 'Sinusonasus', 'Omeisaurus', 'Agujaceratops', 'Aucasaurus', 'Wulatelong', 'Stegopelta', 'Europasaurus', 'Alxasaurus', 'Stegosauroides', 'Tangvayosaurus', 'Pliosaur', 'Unquillosaurus', 'Xixiposaurus', 'Ornithopsis', 'Crataeomus', 'Madsenius', 'Diplotomodon', 'Kulceratops', 'Adamantisaurus', 'Nyororosaurus', 'Monoclonius', 'Kuszholia', 'Jintasaurus', 'Camptosaurus', 'Masiakasaurus', 'Parhabdodon', 'Prodeinodon', 'Saurophagus', 'Alvarezsaurus', 'Mantellisaurus', 'Aristosaurus', 'Suzhousaurus', 'Limnosaurus', 'Dromaeosauroides', 'Augustia', 'Spinosaurid', 'Othnielia', 'Thecospondylus', 'Cetacea', 'Medusaceratops', 'Zalmoxes', 'Koreaceratops', 'Syngonosaurus', 'Dicraeosaurus', 'Xenoceratops', 'Scleromochlus', 'Deinocheirus', 'Chuandongocoelurus', 'Itemirus', 'Phyllodon', 'Megaraptor', 'Philovenator', 'Ferganasaurus', 'Pelorosaurus', 'Nebulasaurus', 'Barilium', 'Nedoceratops', 'Acanthopholis', 'Nuthetes', 'Panoplosaurus', 'Quaesitosaurus', 'Crocodile', 'Archaeoraptor', 'Charonosaurus', 'Eureodon', 'Hanssuesia', 'Guaibasaurus', 'Camposaurus', 'Hylosaurus', 'Zapsalis', 'Bistahieversor', 'Erlicosaurus', 'Procompsognathus', 'Hypselosaurus', 'Prenocephale', 'Malarguesaurus', 'Tenontosaurus', 'Kittysaurus', 'Iguanodon', 'Palaeoctonus', 'Trachodon', 'Wyomingraptor', 'Huehuecanauhtlus', 'Theiophytalia', 'Nectosaurus', 'Scolosaurus', 'Pisanosaurus', 'Kunmingosaurus', 'Liubangosaurus', 'Malawisaurus', 'Pakisaurus', 'Carnosaur', 'Leonerasaurus', 'Oviraptor', 'Texacephale', 'Shantungosaurus', 'Zhuchengosaurus', 'Gripposaurus', 'Polacanthus', 'Xenotarsosaurus', 'Dinosauriform', 'Hypsibema', 'Podokesaurus', 'Sonidosaurus', 'Colepiocephale', 'Kelmayisaurus', 'Crichtonsaurus', 'Stegoceras', 'Borogovia', 'Lourinhasaurus', 'Picrodon', 'Mononychus', 'Bothriospondylus', 'Siamosaurus', 'Technosaurus', 'Theropod', 'Neosodon', 'Velociraptor', 'Eolosaurus', 'Archaeovolans', 'Chebsaurus', 'Compsosuchus', 'Othnielosaurus', 'Banji', 'Proiguanodon', 'Rahonavis', 'Majungasaurus', 'Nanotyrannosaurus', 'Bissektipelta', 'Revueltosaurus', 'Lexovisaurus', 'Antrodemus', 'Hikanodon', 'Tyrannosaurus', 'Barrosasaurus', 'Sinoceratops', 'Megadontosaurus', 'Sinosauropteryx', 'Heilongjiangosaurus', 'Maxakalisaurus', 'Jensenosaurus', 'Dandakosaurus', 'Therosaurus', 'Sarahsaurus', 'Hypselorhachis', 'Archaeodontosaurus', 'Gresslyosaurus', 'Stenopelix']}\n"
     ]
    }
   ],
   "source": [
    "with open('dinosaurs.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d['dinosaurs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dinotext.txt','a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    file.write(i)\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18769 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinotext.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 1, 13, 0, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'a', 'm', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_cell_forward\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    # compute next activation state using the formula given above\n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_forward\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and Wy\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradients of input data, of shape (n_x, m)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from cache\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
    "    dtanh = (1-a_next*a_next)*da_next\n",
    "\n",
    "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
    "    dxt = np.dot(Wax.T,  dtanh)\n",
    "    dWax = np.dot(dtanh,xt.T)\n",
    "\n",
    "    # compute the gradient with respect to Waa (≈2 lines)\n",
    "    da_prev = np.dot(Waa.T, dtanh)  \n",
    "    dWaa = np.dot( dtanh,a_prev.T)\n",
    "\n",
    "    # compute the gradient with respect to b (≈1 line)\n",
    "    dba = np.sum( dtanh,keepdims=True,axis=-1)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
    "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
    "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
    "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
    "    (caches, x) = caches\n",
    "    (a1, a0, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape \n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈6 lines)\n",
    "    dx = np.zeros((n_x, m, T_x)) \n",
    "    dWax = np.zeros((n_a, n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, 1)) \n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))  \n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        # Retrieve derivatives from gradients (≈ 1 line)\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
    "        dx[:, :, t] = dxt  \n",
    "        dWax += dWaxt  \n",
    "        dWaa += dWaat  \n",
    "        dba += dbat  \n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
    "    da0 = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    print(type(X))\n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c591118d1d1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m26\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_last\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradients[\\\"dWaa\\\"][1][2] =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dWaa\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-ae53281de15e>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(X, Y, a_prev, parameters, learning_rate)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Forward propagate through time (≈1 line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Backpropagate through time (≈1 line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-60c4959684b8>\u001b[0m in \u001b[0;36mrnn_forward\u001b[1;34m(x, a0, parameters)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Retrieve dimensions from shapes of x and Wy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mn_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Wya\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-fe6b20764e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-64cac185c107>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(data, ix_to_char, char_to_ix, num_iterations, n_a, dino_names, vocab_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Initialize parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Initialize loss (this is required because we want to smooth our loss, don't worry about it)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shakespeare_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-0fb646a18b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mshakespeare_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shakespeare_utils'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
